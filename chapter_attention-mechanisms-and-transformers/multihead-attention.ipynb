{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshineGreeny/Dive-into-deep-learning-Pytorch/blob/main/chapter_attention-mechanisms-and-transformers/multihead-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5e5b9f",
      "metadata": {
        "id": "2d5e5b9f"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3cf184a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:05.720112Z",
          "iopub.status.busy": "2023-08-18T19:42:05.719157Z",
          "iopub.status.idle": "2023-08-18T19:42:08.696163Z",
          "shell.execute_reply": "2023-08-18T19:42:08.694248Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "3cf184a4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools"
      ],
      "metadata": {
        "id": "yG5ZJt4gWTj0"
      },
      "id": "yG5ZJt4gWTj0"
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "\n",
        "        # 创建掩码：对于每个位置，如果索引小于有效长度则为True，否则为False\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "\n",
        "        # 将掩码为False的位置设置为指定的值\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    # 没有提供有效长度，直接返回标准sofemax\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "\n",
        "        # 处理不同维度的有效长度\n",
        "        if valid_lens.dim() == 1:\n",
        "            # 如果是一维,复制到每个位置\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            # 如果是二维,展平\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "\n",
        "        # 应用序列掩码,将无效位置设置为很小的值\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "\n",
        "        # 应用softmax并恢复原始形状\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
        "\n",
        "\n",
        "def check_shape(tensor, expected_shape):\n",
        "    assert tensor.shape == expected_shape, \\\n",
        "        f'Expected shape: {expected_shape}, got: {tensor.shape}'\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "bM49s_snWVr2"
      },
      "id": "bM49s_snWVr2",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e8ff77b7",
      "metadata": {
        "id": "e8ff77b7"
      },
      "source": [
        "Choose the scaled dot product attention\n",
        "for each head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bdc6ec21",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.702133Z",
          "iopub.status.busy": "2023-08-18T19:42:08.701108Z",
          "iopub.status.idle": "2023-08-18T19:42:08.710324Z",
          "shell.execute_reply": "2023-08-18T19:42:08.709226Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "bdc6ec21"
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        # 获取特征维度（用于缩放）\n",
        "        d = queries.shape[-1]\n",
        "\n",
        "        # 计算注意力分数：Q * K^T / √d\n",
        "        # bmm: 批量矩阵乘法\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "\n",
        "        # 应用带掩码的softmax得到注意力权重\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "\n",
        "        # 对注意力权重应用dropout，然后与值相乘得到最终输出\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = DotProductAttention(dropout)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries = self.transpose_qkv(self.W_q(queries))\n",
        "        keys = self.transpose_qkv(self.W_k(keys))\n",
        "        values = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "        if valid_lens is not None:\n",
        "            valid_lens = torch.repeat_interleave(\n",
        "                valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        output_concat = self.transpose_output(output)\n",
        "        return self.W_o(output_concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50441e5",
      "metadata": {
        "id": "d50441e5"
      },
      "source": [
        "Parallel computation of multiple heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4da9ac2c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.714864Z",
          "iopub.status.busy": "2023-08-18T19:42:08.714207Z",
          "iopub.status.idle": "2023-08-18T19:42:08.723031Z",
          "shell.execute_reply": "2023-08-18T19:42:08.722024Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "4da9ac2c"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "    super().__init__()\n",
        "    self.num_heads=num_heads\n",
        "    self.num_hiddens=num_hiddens\n",
        "    self.W_q=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.W_k=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.W_v=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.W_o=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.attention=DotProductAttention(dropout)\n",
        "\n",
        "  def transpose_qkv(self, X):\n",
        "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
        "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "  def transpose_output(self, X):\n",
        "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
        "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
        "\n",
        "  def forward(self, queries, keys, values, valid_lens):\n",
        "    Q=self.transpose_qkv(self.W_q(queries))\n",
        "    K=self.transpose_qkv(self.W_k(keys))\n",
        "    V=self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "    if valid_lens is not None:\n",
        "      valid_lens=torch.repeat_interleave(valid_lens,repeats=self.num_heads,dim=0)\n",
        "\n",
        "    output=self.attention(Q,K,V,valid_lens)\n",
        "    output=self.transpose_output(output)\n",
        "    return self.W_o(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6644d7",
      "metadata": {
        "id": "6d6644d7"
      },
      "source": [
        "Test our implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "18451bc5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.726610Z",
          "iopub.status.busy": "2023-08-18T19:42:08.726042Z",
          "iopub.status.idle": "2023-08-18T19:42:08.759713Z",
          "shell.execute_reply": "2023-08-18T19:42:08.758787Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "18451bc5",
        "outputId": "e4b2d21c-3b03-4644-8a16-aab9f70ef981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3629,  0.3063,  0.2922,  0.0157,  0.7288,  0.2118, -0.1649,\n",
              "          -0.0513,  0.2093, -0.0042, -0.5062,  0.1646,  0.7694, -0.2548,\n",
              "          -0.1410,  0.1100,  0.5411, -0.2249, -0.5944, -0.3839,  0.3824,\n",
              "           0.2624,  0.8636,  0.0092,  0.2005,  0.0354,  0.2192,  0.2643,\n",
              "           0.4009, -0.2797, -0.2515, -0.2407, -0.0861, -0.1620, -0.1907,\n",
              "           0.5468, -0.1624, -0.2522,  0.1096, -0.3083,  0.0548, -0.2064,\n",
              "           0.3342, -0.0238,  0.0226,  0.2633,  0.0821, -0.3914, -0.5324,\n",
              "          -0.2480, -0.0423,  0.1026, -0.0859, -0.1098, -0.2245,  0.3068,\n",
              "          -0.0097,  0.1018, -0.3227, -0.2032,  0.1347,  0.2842, -0.0829,\n",
              "          -0.1709,  0.4872,  0.2822, -0.2002,  0.5427, -0.3976, -0.3927,\n",
              "          -0.1316,  0.1736,  0.0751, -0.5188, -0.2101,  0.1032,  0.0685,\n",
              "          -0.4454,  0.3371,  0.5203, -0.7297,  0.6100,  0.0792,  0.1182,\n",
              "          -0.1401, -0.4456,  0.2232, -0.1571, -0.2637,  0.1433, -0.1899,\n",
              "           0.2908, -0.2296,  0.4885,  0.2207, -0.0790, -0.0961, -0.2816,\n",
              "           0.3675,  0.7977],\n",
              "         [ 0.1970,  0.1639,  0.3771,  0.2861,  0.6796,  0.1226, -0.2836,\n",
              "           0.0085,  0.4092, -0.0413, -0.6742,  0.2830,  0.6825, -0.0118,\n",
              "          -0.2164, -0.1530,  0.6206, -0.4837, -0.4147, -0.2381,  0.1149,\n",
              "           0.0636,  0.7965, -0.0077,  0.0472,  0.0542,  0.3057, -0.1836,\n",
              "           0.0607, -0.1926, -0.4781, -0.1947, -0.0634, -0.2115, -0.1834,\n",
              "           0.0901,  0.1223, -0.1102,  0.3038, -0.6310, -0.0857, -0.5022,\n",
              "           0.3840, -0.2991, -0.2788,  0.0030,  0.0634,  0.0088, -0.2798,\n",
              "          -0.0245, -0.0413,  0.0965,  0.1133, -0.1827, -0.1965,  0.3043,\n",
              "          -0.1264,  0.1439,  0.1674, -0.1102,  0.0982,  0.5002, -0.2331,\n",
              "          -0.2762,  0.7234,  0.1638, -0.1006,  0.2004, -0.1160, -0.1259,\n",
              "          -0.1067, -0.0930,  0.2602, -0.3777, -0.2734,  0.0298, -0.1597,\n",
              "          -0.5287, -0.1819,  0.4213, -0.7386,  0.5581,  0.1298,  0.2643,\n",
              "          -0.1380, -0.7752,  0.4738, -0.5704,  0.0286, -0.2073, -0.0800,\n",
              "           0.0708, -0.2662,  0.4596,  0.3445, -0.0931, -0.1849, -0.1505,\n",
              "           0.1504,  0.3560],\n",
              "         [-0.0555,  0.2813,  0.4833,  0.2779,  0.8418, -0.0070, -0.1071,\n",
              "          -0.1063,  0.4748,  0.2185, -0.8072,  0.3275,  0.8472, -0.2944,\n",
              "          -0.3557, -0.0708,  0.5658, -0.2852, -0.7376,  0.0039,  0.2773,\n",
              "           0.1488,  0.8552,  0.1808,  0.0869, -0.2453,  0.1772, -0.0857,\n",
              "          -0.2937, -0.1525, -0.4978, -0.2041,  0.0631,  0.0048, -0.2987,\n",
              "          -0.1853,  0.2579, -0.5089,  0.0726, -0.4703,  0.0789, -0.8986,\n",
              "           0.5294, -0.3102, -0.3392,  0.3391, -0.1111, -0.3605, -0.6669,\n",
              "          -0.0046,  0.2049,  0.4870,  0.1740, -0.0090,  0.0445,  0.2703,\n",
              "           0.0928, -0.0434,  0.0711, -0.3350,  0.4761,  0.5347, -0.1480,\n",
              "          -0.4886,  0.6975,  0.3676, -0.4535,  0.4106, -0.4012, -0.3738,\n",
              "          -0.5001,  0.2120,  0.0366, -0.3637, -0.4476,  0.2303,  0.0828,\n",
              "          -0.5581, -0.4454,  0.7343, -0.6810,  0.0439, -0.1963, -0.1629,\n",
              "          -0.4337, -0.7879,  0.3427, -0.5911, -0.1943, -0.3946, -0.4400,\n",
              "           0.0077, -0.0901,  0.4375,  0.0862, -0.0296, -0.5152, -0.2207,\n",
              "           0.2720,  0.4947],\n",
              "         [ 0.1415,  0.0959,  0.1212,  0.0881,  0.2996,  0.0803, -0.2271,\n",
              "           0.0410,  0.1228, -0.1255, -0.2653,  0.1384,  0.1911,  0.1546,\n",
              "          -0.0877, -0.1377,  0.2572, -0.2641, -0.1685, -0.1550, -0.0191,\n",
              "           0.0891,  0.3488, -0.0549,  0.0052,  0.1829,  0.2609, -0.0963,\n",
              "           0.2357, -0.1454, -0.1929, -0.0784, -0.1158, -0.1731,  0.0187,\n",
              "           0.1885, -0.0395,  0.1338,  0.1893, -0.3969, -0.1226, -0.0589,\n",
              "           0.0576, -0.1995, -0.1459, -0.0636,  0.0726,  0.1933, -0.0048,\n",
              "           0.0102, -0.1747, -0.1988,  0.0561, -0.1813, -0.2041,  0.2136,\n",
              "          -0.1826,  0.1737,  0.1124,  0.0843, -0.1786,  0.1857, -0.1376,\n",
              "          -0.0418,  0.2853,  0.0207,  0.1754,  0.0325,  0.0894, -0.0140,\n",
              "           0.0886, -0.2245,  0.1624, -0.1928,  0.1102, -0.0639, -0.1209,\n",
              "          -0.1593,  0.0632,  0.0153, -0.3795,  0.4652,  0.2301,  0.3092,\n",
              "           0.0785, -0.3510,  0.2201, -0.2656,  0.0751, -0.0146,  0.0690,\n",
              "           0.0696, -0.1900,  0.1590,  0.3081, -0.0919,  0.0991, -0.0790,\n",
              "           0.0536,  0.1728]],\n",
              "\n",
              "        [[ 0.3124,  0.2130,  0.2228, -0.0681,  0.5312, -0.0150, -0.0882,\n",
              "          -0.0911,  0.1112, -0.1108, -0.4771,  0.0822,  0.5357, -0.0151,\n",
              "          -0.2012,  0.0778,  0.3548, -0.1156, -0.6914, -0.0617,  0.1460,\n",
              "           0.5349,  0.4878, -0.0294, -0.0275,  0.2304,  0.3036,  0.2390,\n",
              "           0.4277, -0.1430, -0.2556, -0.4121,  0.0745, -0.1968, -0.1374,\n",
              "           0.2485, -0.1648,  0.0376, -0.0677, -0.3961,  0.0543, -0.1306,\n",
              "           0.2420, -0.1118, -0.0836,  0.2397,  0.1536, -0.0482, -0.3793,\n",
              "          -0.2188, -0.0513, -0.0859, -0.0577, -0.1496, -0.1697,  0.2792,\n",
              "          -0.0079,  0.0349, -0.1068,  0.0592, -0.0716,  0.3478, -0.1123,\n",
              "          -0.1666,  0.2669,  0.1613, -0.1400,  0.3832, -0.2103, -0.4347,\n",
              "           0.0163,  0.0016, -0.0793, -0.4463,  0.2004,  0.1610,  0.2299,\n",
              "          -0.2606,  0.4242,  0.3982, -0.6222,  0.4121,  0.1544,  0.1741,\n",
              "          -0.1214, -0.2854,  0.1237, -0.2208, -0.1001,  0.1490, -0.1451,\n",
              "           0.1417, -0.1495,  0.1650,  0.2097,  0.0762,  0.0756, -0.2986,\n",
              "           0.2788,  0.6288],\n",
              "         [ 0.4247,  0.3120,  0.1731, -0.1455,  0.6467,  0.1092, -0.1769,\n",
              "          -0.1410,  0.0482, -0.2238, -0.3656,  0.0581,  0.5737,  0.0085,\n",
              "          -0.1292,  0.1718,  0.3967, -0.2158, -0.7776, -0.3064,  0.2416,\n",
              "           0.6519,  0.6619, -0.0380, -0.0420,  0.2825,  0.3765,  0.4055,\n",
              "           0.7490, -0.2699, -0.1008, -0.3425, -0.0408, -0.4252, -0.0272,\n",
              "           0.6534, -0.4100,  0.1959, -0.0553, -0.3919,  0.1201,  0.1667,\n",
              "           0.2372, -0.0605,  0.0120,  0.3128,  0.2960, -0.1039, -0.3122,\n",
              "          -0.3132, -0.1703, -0.2614, -0.1881, -0.2431, -0.3944,  0.4451,\n",
              "          -0.0742,  0.1697, -0.3738,  0.1199, -0.2765,  0.1628, -0.0744,\n",
              "          -0.0440,  0.2208,  0.1795,  0.0252,  0.5058, -0.1868, -0.3772,\n",
              "           0.1223, -0.0124, -0.0048, -0.6239,  0.4070,  0.1307,  0.2114,\n",
              "          -0.3057,  0.7717,  0.3929, -0.8961,  0.7661,  0.3296,  0.4865,\n",
              "          -0.0338, -0.2440,  0.1770, -0.0276, -0.1078,  0.3904, -0.1705,\n",
              "           0.3745, -0.3953,  0.2555,  0.3718, -0.0386,  0.1889, -0.3912,\n",
              "           0.3011,  1.0231],\n",
              "         [ 0.1271,  0.4753,  0.1030, -0.1356,  0.7050,  0.3183, -0.1738,\n",
              "          -0.1442,  0.0112,  0.0406, -0.0765,  0.1235,  0.4849, -0.3217,\n",
              "          -0.0169,  0.2194,  0.2652, -0.1318, -0.5745, -0.5387,  0.5313,\n",
              "           0.2479,  0.8498,  0.1537,  0.2305, -0.1655,  0.1339,  0.4733,\n",
              "           0.5133, -0.4172,  0.1947,  0.1602, -0.2993, -0.2466,  0.0854,\n",
              "           0.9338, -0.4256, -0.2327, -0.0143,  0.0269,  0.1973,  0.1617,\n",
              "           0.1606,  0.0588,  0.1611,  0.5020,  0.0676, -0.6254, -0.4351,\n",
              "          -0.2075, -0.1301, -0.0037, -0.1912, -0.0872, -0.3502,  0.4325,\n",
              "          -0.0276,  0.2263, -0.7552, -0.2264, -0.0142, -0.2804,  0.1151,\n",
              "           0.0248,  0.1330,  0.3556,  0.0573,  0.6386, -0.3548, -0.2227,\n",
              "          -0.2252,  0.2607,  0.0562, -0.5063,  0.0872,  0.0616,  0.0872,\n",
              "          -0.2584,  0.5201,  0.3488, -0.7194,  0.6039,  0.1130,  0.2630,\n",
              "          -0.0531, -0.1750,  0.0725,  0.2961, -0.3838,  0.3402, -0.3852,\n",
              "           0.5527, -0.4099,  0.4331,  0.2395, -0.3097, -0.0865, -0.3139,\n",
              "           0.3321,  1.1673],\n",
              "         [-0.4880,  0.2473,  0.3361,  0.3257,  0.6666, -0.2234, -0.2032,\n",
              "          -0.2257,  0.4089,  0.0560, -0.5654,  0.3361,  0.3016,  0.1823,\n",
              "          -0.3918, -0.2470,  0.2392, -0.4374, -0.7623,  0.2896, -0.0885,\n",
              "           0.3248,  0.4813,  0.2750, -0.3479, -0.1931,  0.3414, -0.3378,\n",
              "          -0.4148, -0.0944, -0.2898,  0.0433,  0.0369, -0.3043,  0.0656,\n",
              "          -0.5131,  0.2211,  0.0553, -0.0423, -0.6532,  0.1173, -0.7247,\n",
              "           0.3719, -0.6481, -0.6601,  0.3303,  0.0121,  0.2065, -0.1322,\n",
              "           0.2688,  0.0851,  0.1222,  0.2746, -0.1623, -0.0883,  0.4695,\n",
              "          -0.0663,  0.1004,  0.3286,  0.0625, -0.0107,  0.2629, -0.1521,\n",
              "          -0.4030,  0.3955,  0.2057,  0.0050,  0.0315,  0.1532,  0.0382,\n",
              "          -0.3843, -0.1846,  0.0784, -0.2368,  0.3288,  0.1963,  0.0466,\n",
              "          -0.3357, -0.6163,  0.3990, -0.7719, -0.0904,  0.0473,  0.3591,\n",
              "          -0.3376, -0.7561,  0.4258, -0.6485,  0.1997, -0.5077, -0.5242,\n",
              "          -0.0447, -0.3534,  0.0477,  0.3143, -0.1383, -0.4059, -0.1982,\n",
              "          -0.0831,  0.4168]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "num_hiddens, num_heads = 100, 5\n",
        "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
        "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
        "valid_lens = torch.tensor([3, 2])\n",
        "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
        "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
        "check_shape(attention(X, Y, Y, valid_lens),\n",
        "                (batch_size, num_queries, num_hiddens))"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "rise": {
      "autolaunch": true,
      "enable_chalkboard": true,
      "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
      "scroll": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}