{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshineGreeny/Dive-into-deep-learning-Pytorch/blob/colab-experiments/chapter_attention-mechanisms-and-transformers/multihead-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5e5b9f",
      "metadata": {
        "id": "2d5e5b9f"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3cf184a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:05.720112Z",
          "iopub.status.busy": "2023-08-18T19:42:05.719157Z",
          "iopub.status.idle": "2023-08-18T19:42:08.696163Z",
          "shell.execute_reply": "2023-08-18T19:42:08.694248Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "3cf184a4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools"
      ],
      "metadata": {
        "id": "mGzQupCzE5MM"
      },
      "id": "mGzQupCzE5MM"
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "\n",
        "        # 创建掩码：对于每个位置，如果索引小于有效长度则为True，否则为False\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "\n",
        "        # 将掩码为False的位置设置为指定的值\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    # 没有提供有效长度，直接返回标准sofemax\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "\n",
        "        # 处理不同维度的有效长度\n",
        "        if valid_lens.dim() == 1:\n",
        "            # 如果是一维,复制到每个位置\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            # 如果是二维,展平\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "\n",
        "        # 应用序列掩码,将无效位置设置为很小的值\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "\n",
        "        # 应用softmax并恢复原始形状\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
        "\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        # 获取特征维度（用于缩放）\n",
        "        d = queries.shape[-1]\n",
        "\n",
        "        # 计算注意力分数：Q * K^T / √d\n",
        "        # bmm: 批量矩阵乘法\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "\n",
        "        # 应用带掩码的softmax得到注意力权重\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "\n",
        "        # 对注意力权重应用dropout，然后与值相乘得到最终输出\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
        "\n",
        "def check_shape(tensor, expected_shape):\n",
        "    assert tensor.shape == expected_shape, \\\n",
        "        f'Expected shape: {expected_shape}, got: {tensor.shape}'\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "Nx03UVinFBzk"
      },
      "id": "Nx03UVinFBzk",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e8ff77b7",
      "metadata": {
        "id": "e8ff77b7"
      },
      "source": [
        "Choose the scaled dot product attention\n",
        "for each head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bdc6ec21",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.702133Z",
          "iopub.status.busy": "2023-08-18T19:42:08.701108Z",
          "iopub.status.idle": "2023-08-18T19:42:08.710324Z",
          "shell.execute_reply": "2023-08-18T19:42:08.709226Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "bdc6ec21"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = DotProductAttention(dropout)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries = self.transpose_qkv(self.W_q(queries))\n",
        "        keys = self.transpose_qkv(self.W_k(keys))\n",
        "        values = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "        if valid_lens is not None:\n",
        "            valid_lens = torch.repeat_interleave(\n",
        "                valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        output_concat = self.transpose_output(output)\n",
        "        return self.W_o(output_concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50441e5",
      "metadata": {
        "id": "d50441e5"
      },
      "source": [
        "Parallel computation of multiple heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4da9ac2c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.714864Z",
          "iopub.status.busy": "2023-08-18T19:42:08.714207Z",
          "iopub.status.idle": "2023-08-18T19:42:08.723031Z",
          "shell.execute_reply": "2023-08-18T19:42:08.722024Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "4da9ac2c"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.num_hiddens = num_hiddens\n",
        "        # 线性映射 qkv 和输出\n",
        "        self.W_q = nn.Linear(num_hiddens, num_hiddens)\n",
        "        self.W_k = nn.Linear(num_hiddens, num_hiddens)\n",
        "        self.W_v = nn.Linear(num_hiddens, num_hiddens)\n",
        "        self.W_o = nn.Linear(num_hiddens, num_hiddens)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attention = DotProductAttention(dropout) # Add attention module\n",
        "\n",
        "    def transpose_qkv(self, X):\n",
        "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
        "        # X: (batch_size, seq_len, num_hiddens)\n",
        "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "        X = X.permute(0, 2, 1, 3)  # (B, num_heads, seq_len, head_dim)\n",
        "        return X.reshape(-1, X.shape[2], X.shape[3])  # 合并 batch 和 head\n",
        "\n",
        "    def transpose_output(self, X):\n",
        "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
        "        # X: (B*num_heads, seq_len, head_dim)\n",
        "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "        X = X.permute(0, 2, 1, 3)  # (B, seq_len, num_heads, head_dim)\n",
        "        return X.reshape(X.shape[0], X.shape[1], -1)  # 合并 head_dim\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        # 假设 queries/keys/values 都是 (B, T, num_hiddens)\n",
        "        Q = self.transpose_qkv(self.W_q(queries))\n",
        "        K = self.transpose_qkv(self.W_k(keys))\n",
        "        V = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "        # Repeat valid_lens for each head\n",
        "        if valid_lens is not None:\n",
        "             # valid_lens shape: (batch_size,)\n",
        "             # Repeat for each head\n",
        "             valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
        "             # valid_lens shape after repeat: (batch_size * num_heads,)\n",
        "\n",
        "\n",
        "        # Here is the scaled dot-product attention\n",
        "        output = self.attention(Q, K, V, valid_lens) # Use the attention module\n",
        "        output = self.transpose_output(output)\n",
        "        return self.W_o(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6644d7",
      "metadata": {
        "id": "6d6644d7"
      },
      "source": [
        "Test our implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "18451bc5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.726610Z",
          "iopub.status.busy": "2023-08-18T19:42:08.726042Z",
          "iopub.status.idle": "2023-08-18T19:42:08.759713Z",
          "shell.execute_reply": "2023-08-18T19:42:08.758787Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18451bc5",
        "outputId": "1c338aef-0100-4b7b-bf30-81c9b1025dd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-5.0682e-01, -9.1867e-02, -9.6111e-02,  3.5473e-01,  4.1095e-01,\n",
              "           5.9812e-01, -1.0575e-01, -1.7675e-01,  4.7958e-01,  4.4001e-01,\n",
              "          -6.4413e-01,  6.1430e-01,  1.9326e-01, -2.2981e-02, -2.0853e-01,\n",
              "          -5.9309e-02,  9.8570e-01,  4.1541e-01, -8.1727e-01,  2.6197e-01,\n",
              "           4.5640e-01,  4.1301e-02,  3.0625e-01, -4.3465e-01,  5.2807e-02,\n",
              "           7.6951e-02,  2.5736e-01,  7.4846e-02, -3.3225e-01,  3.3010e-01,\n",
              "          -4.2965e-01,  3.9745e-01, -1.3902e-01, -3.9534e-01, -6.4195e-01,\n",
              "          -1.7562e-01,  2.5535e-01,  6.6362e-01,  5.9950e-01, -2.9680e-01,\n",
              "           4.2467e-01,  7.9417e-02, -5.6159e-01,  2.3377e-01, -2.9319e-01,\n",
              "          -7.0728e-01,  2.0414e-01, -2.5556e-01,  3.5403e-01, -3.0231e-01,\n",
              "          -3.6119e-02,  6.9782e-01, -1.2039e+00, -1.9244e-03, -2.7149e-01,\n",
              "          -2.3990e-01,  2.7966e-01, -2.2049e-01, -4.8323e-02, -9.8118e-01,\n",
              "           2.1713e-02,  2.7151e-01, -3.9364e-01, -7.9916e-01, -1.3124e-01,\n",
              "           1.4663e-01,  2.5815e-01,  2.2463e-01, -1.3956e-01, -5.2539e-02,\n",
              "           4.3558e-02,  3.3880e-01, -5.1888e-01,  3.8284e-01, -8.3806e-01,\n",
              "          -6.5967e-02, -1.8304e-01, -9.5093e-02,  1.5219e-02, -5.4187e-01,\n",
              "          -2.5264e-01,  6.3077e-03, -7.7087e-01,  3.7316e-02,  6.0326e-03,\n",
              "           5.6019e-01,  3.9420e-02,  1.2285e-01, -2.4795e-01,  1.9658e-02,\n",
              "          -4.9208e-01, -4.9069e-01,  1.4330e-01,  2.1686e-01,  3.5634e-01,\n",
              "           3.2435e-01, -5.0635e-01,  1.8457e-01, -5.4008e-02,  2.5960e-01],\n",
              "         [-3.2915e-01, -3.6315e-01, -6.9910e-01,  1.6672e-01,  1.1385e-01,\n",
              "           3.0519e-01, -2.2527e-01, -8.3133e-02,  4.8212e-01,  5.7486e-01,\n",
              "          -2.9810e-01,  4.4483e-01,  3.5419e-02, -5.4866e-01, -1.2831e-01,\n",
              "           2.5370e-01,  5.0425e-01,  4.7267e-01, -7.9752e-01,  6.3873e-02,\n",
              "           1.1862e-01,  1.2421e-01,  6.5914e-02,  5.9632e-02, -1.5040e-02,\n",
              "           1.0161e-01,  4.3709e-01,  1.1480e-01, -7.8207e-02,  2.5829e-01,\n",
              "          -4.7298e-01,  5.1756e-01, -6.0772e-02, -3.9172e-02, -2.5875e-01,\n",
              "          -1.7203e-01, -7.3329e-03,  1.5935e-01,  4.8735e-01, -5.5173e-01,\n",
              "          -1.1790e-01, -7.3650e-02, -3.4476e-01,  2.3157e-01, -4.5044e-01,\n",
              "          -2.7499e-02, -3.4923e-01,  1.6011e-01,  1.2194e-01,  2.1388e-01,\n",
              "           3.3854e-01,  2.3726e-02, -7.0398e-01, -1.0107e-01, -2.6295e-01,\n",
              "          -1.8426e-01,  2.6759e-01, -4.1147e-01,  8.1697e-02, -3.7693e-01,\n",
              "          -4.6313e-01,  3.0441e-01, -5.6046e-01, -3.3006e-01,  2.1564e-01,\n",
              "           1.9435e-01,  6.2689e-01,  6.1555e-02, -4.6238e-02,  1.0243e-01,\n",
              "          -2.8000e-01,  1.8397e-01,  1.5976e-01,  2.3461e-01, -6.2015e-01,\n",
              "           5.4763e-01, -1.9414e-01, -1.6314e-02, -1.5735e-01, -4.5895e-01,\n",
              "           2.5767e-01,  7.8935e-02, -3.8055e-01, -1.2754e-01,  6.6356e-02,\n",
              "          -4.2173e-01,  4.1601e-01,  5.5879e-01,  7.8371e-02, -1.8199e-02,\n",
              "          -2.1144e-01, -2.8799e-01,  1.0902e-01, -4.4287e-03,  7.4844e-01,\n",
              "          -2.0104e-02, -3.9313e-01,  3.0298e-01, -3.5922e-02,  3.1657e-01],\n",
              "         [-3.1343e-01, -1.7203e-01,  2.0115e-01,  1.7633e-03,  1.4778e-02,\n",
              "           1.4368e-01, -3.6020e-01, -2.9013e-01,  4.2929e-01,  5.4061e-01,\n",
              "          -4.1723e-01,  6.2779e-01,  5.0875e-02, -1.1537e-01, -3.0504e-01,\n",
              "           5.3625e-01,  7.4059e-01,  3.0966e-01, -3.4948e-01,  4.2581e-01,\n",
              "           1.5415e-01,  7.3661e-03,  3.8483e-01, -4.3573e-01, -3.6933e-02,\n",
              "          -2.9325e-01,  3.4968e-01, -1.1649e-01, -3.9992e-01,  2.2612e-01,\n",
              "          -2.0359e-01,  2.6766e-01, -1.3314e-01, -9.4306e-02, -4.3747e-01,\n",
              "          -2.1212e-01, -1.5164e-01,  3.2567e-01,  4.3837e-02,  8.1092e-03,\n",
              "           1.5091e-01, -7.2387e-03, -6.1952e-01, -3.0206e-01, -3.8826e-01,\n",
              "           2.4429e-01,  3.7153e-02,  1.1249e-01, -1.3047e-01, -1.4467e-01,\n",
              "          -2.1627e-02,  7.3627e-02, -1.4888e-01, -1.8772e-01, -3.4913e-01,\n",
              "           1.2921e-01,  3.0810e-01,  1.5414e-01,  1.2121e-01, -2.1791e-01,\n",
              "          -3.7913e-03, -7.4043e-02,  8.1881e-02, -3.9664e-01, -2.8270e-01,\n",
              "          -2.3064e-02,  3.2839e-01,  7.0783e-01, -4.2938e-02, -8.0649e-02,\n",
              "           1.1861e-02,  3.0642e-01, -9.8563e-02, -1.1445e-01, -6.5616e-01,\n",
              "           5.1569e-01, -3.4295e-01,  1.0819e-01,  6.5035e-01, -2.1438e-01,\n",
              "          -9.1012e-01, -2.2000e-01, -6.5279e-01, -1.1052e-01, -5.7829e-03,\n",
              "           1.1468e-01, -2.2297e-01,  2.0091e-01,  7.0955e-02,  2.5987e-01,\n",
              "          -2.6978e-01, -2.4124e-01,  1.2916e-01,  3.6727e-01,  2.6084e-01,\n",
              "           1.6575e-01, -2.4947e-01,  4.6006e-01,  2.5740e-01,  2.7712e-01],\n",
              "         [-5.3125e-01, -3.8685e-01,  2.0501e-02,  1.1505e-01,  5.4749e-01,\n",
              "           4.4900e-01, -2.3956e-01, -1.7509e-01,  4.5818e-01,  3.9874e-01,\n",
              "          -5.4912e-01,  5.3742e-01,  1.3731e-01, -1.6389e-03, -1.3449e-01,\n",
              "           9.8156e-02,  1.0901e+00,  6.3415e-01, -6.1644e-01,  5.5857e-01,\n",
              "           3.6702e-01, -1.0507e-01,  3.8945e-01, -7.8286e-01, -4.7095e-02,\n",
              "          -2.6919e-01,  2.6635e-01, -3.8208e-02, -8.9933e-02,  4.1754e-01,\n",
              "          -2.7512e-01,  9.9543e-02, -2.0603e-01, -3.2722e-01, -4.5579e-01,\n",
              "          -2.8560e-01,  3.6129e-01,  6.3012e-01,  2.6620e-01, -2.0986e-01,\n",
              "           6.1129e-01,  2.4115e-01, -6.7696e-01, -3.9214e-02, -4.1419e-01,\n",
              "          -7.4521e-01,  2.1065e-01, -6.8363e-02,  1.8094e-01, -1.5686e-01,\n",
              "           8.4953e-02,  3.3733e-01, -8.9432e-01,  5.1196e-02, -4.3615e-01,\n",
              "          -1.8023e-01,  5.2963e-01, -1.0535e-01, -9.8123e-02, -1.1666e+00,\n",
              "           7.0639e-02,  2.7872e-01, -3.0379e-01, -5.8218e-01, -5.2305e-02,\n",
              "          -6.2386e-02,  3.3535e-01,  1.0161e-01, -1.2138e-01, -2.6637e-01,\n",
              "           1.0363e-01,  1.3494e-01, -6.2917e-01,  3.9621e-01, -1.0423e+00,\n",
              "           8.5850e-03, -2.8756e-01,  1.9536e-01, -5.6590e-03, -5.7419e-01,\n",
              "          -5.6624e-01,  2.9979e-01, -9.9784e-01, -2.5615e-01, -7.1680e-02,\n",
              "           7.7148e-01, -9.1720e-02,  1.7196e-01, -2.1660e-01,  6.2289e-02,\n",
              "          -7.0195e-01, -2.7769e-01,  1.1394e-03,  2.4359e-01,  3.0440e-01,\n",
              "           3.7429e-01, -4.1424e-01,  1.4204e-01, -4.2825e-02,  3.6201e-01]],\n",
              "\n",
              "        [[-2.6639e-01, -6.2037e-01, -9.6371e-01, -5.2401e-02, -8.4681e-02,\n",
              "           1.9646e-01, -2.9042e-01,  1.7123e-01,  2.5061e-01,  4.7131e-01,\n",
              "          -1.6454e-01,  2.2421e-01, -6.0283e-02, -8.7349e-01, -2.4315e-01,\n",
              "           4.6966e-01,  1.3404e-01,  4.4126e-01, -6.6898e-01,  6.3128e-02,\n",
              "          -1.8684e-01,  2.0062e-01, -2.9228e-01,  2.0291e-01, -1.1888e-01,\n",
              "          -1.2671e-01,  5.1223e-01,  1.4199e-01,  2.2402e-01,  1.6337e-01,\n",
              "          -4.7920e-01,  1.3480e-01, -6.3953e-03,  3.2264e-01,  9.7250e-02,\n",
              "          -3.7455e-01,  1.2934e-01, -1.1039e-01,  3.2965e-01, -6.6045e-01,\n",
              "          -2.5536e-01, -5.7024e-02, -1.5116e-01,  1.1877e-01, -4.8480e-01,\n",
              "          -5.5840e-02, -5.0378e-01,  4.4447e-01,  3.7190e-02,  6.9617e-01,\n",
              "           4.8569e-01, -3.7396e-01, -3.2683e-01, -1.1078e-01, -3.4095e-01,\n",
              "          -5.3336e-02,  3.7559e-01, -5.7735e-01,  1.6698e-01, -2.2908e-01,\n",
              "          -4.9901e-01,  4.0824e-01, -7.9434e-01,  1.0284e-01,  3.1411e-01,\n",
              "           1.0494e-01,  6.5298e-01, -1.5032e-01, -1.0479e-01,  1.2468e-01,\n",
              "          -2.8275e-01,  2.7185e-02,  3.9058e-01,  1.4561e-01, -4.3533e-01,\n",
              "           8.3657e-01, -2.4037e-01,  9.5433e-02, -4.2551e-01, -4.9636e-01,\n",
              "           5.1050e-01,  1.8777e-01, -1.1193e-01, -3.0266e-01,  1.1444e-01,\n",
              "          -7.9637e-01,  5.6910e-01,  9.6562e-01,  3.1637e-01, -8.9788e-02,\n",
              "          -2.7216e-04,  2.6333e-02, -1.1315e-02, -1.1958e-01,  9.9061e-01,\n",
              "          -2.3605e-01, -2.0241e-01,  3.4714e-01, -1.4155e-01,  2.8743e-01],\n",
              "         [-1.4683e-01, -1.8197e-01, -3.0908e-01,  2.5410e-01,  6.6147e-01,\n",
              "           5.1356e-02, -4.8713e-02, -4.8343e-01,  7.8919e-01,  5.1089e-01,\n",
              "           3.2716e-02,  2.5573e-01,  1.0908e-01,  6.0514e-02,  5.9553e-01,\n",
              "          -2.7711e-01,  7.6515e-01,  6.8568e-01, -4.7993e-01, -1.5433e-02,\n",
              "           3.6938e-01, -3.3095e-01,  6.3066e-01, -2.2551e-01,  2.7168e-02,\n",
              "           4.0380e-01,  4.5044e-02, -7.7032e-02,  1.9970e-02,  4.4175e-01,\n",
              "          -4.5249e-02,  9.3425e-01, -1.6682e-01, -5.9194e-01, -3.2077e-01,\n",
              "           3.4621e-01, -2.5429e-01,  2.3241e-01,  3.4439e-01, -3.2397e-01,\n",
              "          -1.3047e-02, -3.2624e-02, -4.4114e-01,  4.5312e-01, -3.6328e-01,\n",
              "           2.6410e-01, -4.0146e-01, -7.4710e-02,  8.6472e-03, -4.0311e-01,\n",
              "           4.0848e-01,  3.3726e-02, -8.4318e-01,  1.6383e-01,  1.7115e-02,\n",
              "          -5.9568e-01,  1.5651e-01, -7.1827e-02, -1.9012e-01, -4.1983e-01,\n",
              "          -6.7268e-01,  1.0473e-01,  8.9060e-02, -4.5081e-01,  5.3906e-01,\n",
              "           1.7555e-01,  5.9713e-01, -2.4334e-01,  2.2356e-01, -1.2615e-01,\n",
              "          -4.4411e-01, -8.1859e-02, -4.1847e-02,  4.8922e-01, -6.6718e-01,\n",
              "          -1.0300e-01, -4.5031e-04,  6.0626e-02, -1.8521e-01, -2.4723e-01,\n",
              "           2.0867e-01,  5.0107e-01, -6.4687e-01, -1.0410e-01, -1.4477e-01,\n",
              "           1.5202e-01,  1.6928e-01, -2.8033e-01, -2.8017e-01,  6.4357e-03,\n",
              "          -6.7371e-01, -4.9231e-01,  1.2657e-01, -3.6742e-02,  1.1817e-01,\n",
              "           3.1787e-01, -4.6675e-01, -1.9032e-01,  1.1042e-01,  3.1779e-01],\n",
              "         [-3.5368e-01, -8.1805e-01, -6.0311e-01, -2.5929e-01,  1.0102e-01,\n",
              "           2.2564e-01, -3.1713e-01,  3.2774e-01,  1.1534e-02,  2.0488e-01,\n",
              "          -2.3897e-01,  6.9010e-02, -4.9705e-02, -6.8396e-01, -3.2486e-01,\n",
              "           4.6690e-01,  2.4883e-01,  5.0772e-01, -3.9311e-01,  4.1062e-01,\n",
              "          -2.4738e-01,  7.6713e-02, -3.6359e-01, -3.2178e-01, -2.2776e-01,\n",
              "          -5.7753e-01,  3.9931e-01,  4.8932e-02,  4.8304e-01,  1.9439e-01,\n",
              "          -3.1154e-01, -5.3141e-01, -6.7813e-02,  3.8569e-01,  2.4947e-01,\n",
              "          -6.1471e-01,  5.5495e-01,  3.3942e-02,  4.2836e-02, -4.7567e-01,\n",
              "           2.0420e-01,  2.0017e-01, -2.1219e-01, -1.4497e-01, -4.4494e-01,\n",
              "          -6.9242e-01, -1.7705e-01,  4.7202e-01,  3.7673e-02,  7.8769e-01,\n",
              "           3.9001e-01, -4.2002e-01, -1.9007e-01,  2.6135e-02, -5.0522e-01,\n",
              "           3.5256e-02,  6.4411e-01, -4.8489e-01,  8.6715e-02, -7.2578e-01,\n",
              "          -9.3705e-02,  4.7720e-01, -7.8263e-01,  2.7495e-01,  1.8995e-01,\n",
              "          -1.6365e-01,  3.9315e-01, -3.4092e-01, -2.1529e-01, -1.4147e-01,\n",
              "           2.5057e-02, -1.5483e-01, -5.2626e-02,  2.1461e-01, -5.5542e-01,\n",
              "           5.8182e-01, -3.2459e-01,  3.2942e-01, -5.8047e-01, -6.1036e-01,\n",
              "           1.3627e-01,  4.6043e-01, -3.3055e-01, -5.2064e-01,  4.5310e-02,\n",
              "          -1.4061e-01,  2.9521e-01,  9.6634e-01,  2.6742e-01, -1.0642e-01,\n",
              "          -1.9272e-01,  2.9702e-01, -1.9637e-01, -3.4286e-02,  8.0991e-01,\n",
              "          -9.9812e-02, -4.6595e-02,  2.1429e-01, -2.5267e-01,  2.6567e-01],\n",
              "         [-4.6552e-01, -9.2021e-01, -8.4381e-01, -1.1828e-01,  3.9875e-01,\n",
              "           2.4861e-01, -3.9705e-01,  3.3819e-02,  4.5440e-01,  5.2423e-01,\n",
              "          -2.2960e-01,  3.3082e-01, -3.7045e-02, -6.7863e-01, -4.4511e-02,\n",
              "           3.9623e-01,  6.9207e-01,  9.0014e-01, -7.5322e-01,  4.7242e-01,\n",
              "           1.8475e-02, -2.1664e-02,  6.2972e-02, -3.8995e-01, -1.5802e-01,\n",
              "          -3.4372e-01,  5.2441e-01,  5.1882e-02,  3.8793e-01,  4.2843e-01,\n",
              "          -4.0515e-01,  7.5462e-02, -1.5052e-01,  7.7974e-02,  3.0245e-02,\n",
              "          -4.1381e-01,  3.3011e-01,  1.5608e-01,  1.9174e-01, -6.4797e-01,\n",
              "           1.9864e-01,  2.0335e-01, -4.7787e-01, -1.4293e-02, -6.7844e-01,\n",
              "          -4.2027e-01, -3.7868e-01,  4.2782e-01,  2.2610e-02,  5.7866e-01,\n",
              "           6.1820e-01, -4.3776e-01, -5.7189e-01,  6.6550e-03, -5.5836e-01,\n",
              "          -1.8860e-01,  7.0031e-01, -4.8358e-01, -1.6400e-02, -9.3197e-01,\n",
              "          -4.5869e-01,  5.1920e-01, -7.6967e-01, -6.2455e-02,  4.5313e-01,\n",
              "          -6.1070e-02,  8.4209e-01, -3.4705e-01, -7.6080e-02, -1.8065e-01,\n",
              "          -2.1972e-01, -1.2871e-01, -5.1181e-02,  4.2972e-01, -1.0410e+00,\n",
              "           6.4742e-01, -3.3515e-01,  3.9075e-01, -5.0507e-01, -6.9085e-01,\n",
              "           1.3426e-01,  6.3595e-01, -7.3151e-01, -6.0099e-01, -1.6915e-02,\n",
              "          -1.5304e-01,  4.4285e-01,  8.2539e-01,  9.7348e-02, -6.3533e-02,\n",
              "          -5.6595e-01,  9.4972e-03, -1.2952e-01, -6.8400e-02,  8.9796e-01,\n",
              "           2.1351e-02, -3.4522e-01,  2.1403e-01, -1.7421e-01,  5.1481e-01]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "num_hiddens, num_heads = 100, 5\n",
        "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
        "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
        "valid_lens = torch.tensor([3, 2])\n",
        "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
        "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
        "check_shape(attention(X, Y, Y, valid_lens),\n",
        "                (batch_size, num_queries, num_hiddens))"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "rise": {
      "autolaunch": true,
      "enable_chalkboard": true,
      "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
      "scroll": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}