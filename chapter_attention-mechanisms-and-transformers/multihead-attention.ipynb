{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d5e5b9f",
      "metadata": {
        "id": "2d5e5b9f"
      },
      "source": [
        "# Multi-Head Attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3cf184a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:05.720112Z",
          "iopub.status.busy": "2023-08-18T19:42:05.719157Z",
          "iopub.status.idle": "2023-08-18T19:42:08.696163Z",
          "shell.execute_reply": "2023-08-18T19:42:08.694248Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "3cf184a4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools"
      ],
      "metadata": {
        "id": "f0wlaRg88T51"
      },
      "id": "f0wlaRg88T51"
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "\n",
        "        # 创建掩码：对于每个位置，如果索引小于有效长度则为True，否则为False\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "\n",
        "        # 将掩码为False的位置设置为指定的值\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    # 没有提供有效长度，直接返回标准sofemax\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "\n",
        "        # 处理不同维度的有效长度\n",
        "        if valid_lens.dim() == 1:\n",
        "            # 如果是一维,复制到每个位置\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            # 如果是二维,展平\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "\n",
        "        # 应用序列掩码,将无效位置设置为很小的值\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "\n",
        "        # 应用softmax并恢复原始形状\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
        "\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        # 获取特征维度（用于缩放）\n",
        "        d = queries.shape[-1]\n",
        "\n",
        "        # 计算注意力分数：Q * K^T / √d\n",
        "        # bmm: 批量矩阵乘法\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "\n",
        "        # 应用带掩码的softmax得到注意力权重\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "\n",
        "        # 对注意力权重应用dropout，然后与值相乘得到最终输出\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
        "\n",
        "def check_shape(tensor, expected_shape):\n",
        "    assert tensor.shape == expected_shape, \\\n",
        "        f'Expected shape: {expected_shape}, got: {tensor.shape}'\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "cKe2auBL8VJR"
      },
      "id": "cKe2auBL8VJR",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e8ff77b7",
      "metadata": {
        "id": "e8ff77b7"
      },
      "source": [
        "Choose the scaled dot product attention\n",
        "for each head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bdc6ec21",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.702133Z",
          "iopub.status.busy": "2023-08-18T19:42:08.701108Z",
          "iopub.status.idle": "2023-08-18T19:42:08.710324Z",
          "shell.execute_reply": "2023-08-18T19:42:08.709226Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "bdc6ec21"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = DotProductAttention(dropout)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries = self.transpose_qkv(self.W_q(queries))\n",
        "        keys = self.transpose_qkv(self.W_k(keys))\n",
        "        values = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "        if valid_lens is not None:\n",
        "            valid_lens = torch.repeat_interleave(\n",
        "                valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        output_concat = self.transpose_output(output)\n",
        "        return self.W_o(output_concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50441e5",
      "metadata": {
        "id": "d50441e5"
      },
      "source": [
        "Parallel computation of multiple heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4da9ac2c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.714864Z",
          "iopub.status.busy": "2023-08-18T19:42:08.714207Z",
          "iopub.status.idle": "2023-08-18T19:42:08.723031Z",
          "shell.execute_reply": "2023-08-18T19:42:08.722024Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "4da9ac2c"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "    super().__init__()\n",
        "    self.num_heads=num_heads\n",
        "    self.num_hiddens=num_hiddens\n",
        "    self.W_q=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.W_k=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.W_v=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.W_o=nn.Linear(num_hiddens,num_hiddens)\n",
        "    self.attention=DotProductAttention(dropout)\n",
        "\n",
        "  def transpose_qkv(self, X):\n",
        "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
        "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "  def transpose_output(self, X):\n",
        "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
        "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
        "\n",
        "  def forward(self, queries, keys, values, valid_lens):\n",
        "    Q=self.transpose_qkv(self.W_q(queries))\n",
        "    K=self.transpose_qkv(self.W_k(keys))\n",
        "    V=self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "    if valid_lens is not None:\n",
        "      valid_lens=torch.repeat_interleave(valid_lens,repeats=self.num_heads,dim=0)\n",
        "\n",
        "    output=self.attention(Q,K,V,valid_lens)\n",
        "    output=self.transpose_output(output)\n",
        "    return self.W_o(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6644d7",
      "metadata": {
        "id": "6d6644d7"
      },
      "source": [
        "Test our implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "18451bc5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:42:08.726610Z",
          "iopub.status.busy": "2023-08-18T19:42:08.726042Z",
          "iopub.status.idle": "2023-08-18T19:42:08.759713Z",
          "shell.execute_reply": "2023-08-18T19:42:08.758787Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "18451bc5",
        "outputId": "f53c63d4-5acb-4530-eba0-43e68712bb69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 3.1588e-01, -5.3620e-02, -5.3872e-01,  1.3297e-02,  8.4461e-01,\n",
              "           3.7934e-01,  3.3786e-01,  3.6676e-01, -2.4644e-01,  6.9524e-01,\n",
              "          -5.3751e-01, -1.2681e+00, -2.4270e-01,  5.9949e-01,  2.0265e-01,\n",
              "           1.6614e-01,  3.2251e-01,  5.8388e-01,  7.1025e-01,  1.0319e-01,\n",
              "           8.9562e-01, -3.3539e-01, -1.5246e-01,  5.6413e-01,  5.2562e-02,\n",
              "           5.7807e-01,  1.2123e-01,  3.6487e-01,  4.5249e-01, -5.4267e-01,\n",
              "          -7.8027e-02,  3.3589e-01, -4.2924e-01,  4.9593e-01,  2.6356e-01,\n",
              "          -4.8317e-01,  1.1016e+00, -4.1329e-02,  2.3063e-01,  1.2471e-01,\n",
              "          -5.8109e-01, -2.8962e-01, -6.7634e-01, -7.4485e-01, -5.7372e-01,\n",
              "          -4.7555e-01, -4.3039e-01,  8.9539e-01, -1.3001e-01, -4.7291e-01,\n",
              "          -7.2822e-01,  6.3764e-01,  3.0498e-01, -3.5024e-01, -9.9975e-02,\n",
              "          -6.7348e-01, -4.3543e-01,  5.3985e-01, -5.5539e-01, -4.9863e-01,\n",
              "          -1.6256e-01,  2.2108e-01,  1.0541e+00, -8.7572e-01, -3.2463e-01,\n",
              "           3.5782e-02,  2.5385e-01, -3.3768e-01, -6.7943e-01, -4.9481e-01,\n",
              "           2.3814e-01,  2.7791e-01, -8.4364e-02,  5.4490e-01,  8.7941e-02,\n",
              "           1.8449e-03, -5.2498e-01, -4.8845e-01, -4.0591e-01,  2.1004e-01,\n",
              "          -8.1476e-02, -1.1501e-01, -2.5271e-01,  3.9556e-01,  3.6416e-01,\n",
              "          -5.7692e-01, -2.7708e-01, -4.7944e-01, -5.7090e-01,  3.4163e-02,\n",
              "          -2.7498e-01,  6.7828e-02, -3.0180e-01,  7.4359e-02,  2.8553e-01,\n",
              "          -3.7922e-01,  6.2958e-01,  1.9399e-01,  2.6429e-01, -4.3268e-01],\n",
              "         [ 3.4117e-01,  1.2507e-01, -2.5108e-01,  7.7027e-02,  8.0592e-01,\n",
              "           5.8139e-01,  1.7463e-01,  3.6476e-01, -8.1194e-02,  6.6274e-01,\n",
              "          -4.0861e-01, -1.2590e+00, -3.2304e-01,  4.6599e-01,  2.0073e-01,\n",
              "           1.1162e-01,  4.3768e-01,  6.4491e-01,  6.7608e-01,  7.6332e-02,\n",
              "           6.9186e-01,  4.5865e-03, -5.6757e-01,  5.3690e-01, -1.3173e-01,\n",
              "           6.0705e-01, -3.0415e-03,  2.8480e-01,  3.6544e-01, -4.0116e-01,\n",
              "          -6.8681e-02,  2.9088e-01, -9.3478e-01,  2.0370e-01,  3.7736e-01,\n",
              "          -1.7245e-01,  1.0670e+00, -2.4951e-01,  3.4694e-02,  4.3567e-02,\n",
              "          -8.5595e-01, -3.1997e-01, -4.4472e-01, -5.8873e-01, -4.1274e-01,\n",
              "          -7.0645e-02, -6.7680e-01,  7.2110e-01,  2.8091e-02, -3.9867e-01,\n",
              "          -7.5938e-01,  6.3760e-01,  1.8961e-02, -4.6234e-01, -1.8377e-01,\n",
              "          -7.5365e-01, -3.6320e-01,  9.8848e-02, -4.3089e-01, -2.9770e-01,\n",
              "          -2.1467e-01,  3.5327e-01,  9.6085e-01, -6.4634e-01, -5.2409e-01,\n",
              "          -1.6301e-03, -7.7254e-02, -4.3575e-01, -7.6846e-01, -4.7040e-01,\n",
              "           5.4448e-01,  2.5293e-01, -3.6268e-02,  1.2325e-01,  1.6877e-01,\n",
              "          -1.9570e-04, -5.2084e-01, -5.4843e-01, -1.4556e-01,  4.9853e-01,\n",
              "           1.3784e-01,  1.8136e-01, -1.4347e-01,  2.8945e-01,  4.4946e-01,\n",
              "          -5.2852e-01, -1.5825e-01, -5.2366e-01, -3.4136e-01, -5.2163e-01,\n",
              "          -5.4562e-01,  6.8010e-02, -9.5592e-02,  3.7647e-02,  1.1602e-01,\n",
              "          -4.1158e-01,  8.1909e-01, -7.9194e-02,  3.1231e-01, -3.3689e-01],\n",
              "         [-1.3554e-01,  9.7503e-01,  9.8238e-02,  3.7305e-01,  4.8453e-01,\n",
              "           4.1352e-01, -4.7538e-02,  2.6234e-01,  1.8243e-01,  5.1899e-01,\n",
              "          -5.0828e-01, -1.0415e+00, -1.6745e-01,  2.1308e-01, -8.9014e-02,\n",
              "           1.0800e-01,  2.9751e-01,  1.6888e-01,  3.3930e-01,  3.5903e-01,\n",
              "          -1.0675e-01,  5.3882e-01, -6.7227e-01,  6.2956e-01, -2.4165e-01,\n",
              "           5.2368e-01, -2.4306e-01,  1.1272e-01,  5.0992e-01, -9.1700e-02,\n",
              "          -3.2832e-01, -2.7547e-01, -1.1019e+00, -1.1421e-03,  3.5336e-01,\n",
              "           3.0445e-01,  6.0757e-01, -3.4824e-02, -4.4167e-01,  3.5534e-02,\n",
              "          -9.4769e-01, -4.9462e-01,  2.3692e-02, -1.0966e-01, -1.7639e-01,\n",
              "           1.1255e-01, -5.0650e-01,  1.8664e-01,  3.4730e-02, -2.7455e-01,\n",
              "          -4.4306e-01, -2.5455e-03, -2.0847e-01, -9.1010e-02, -1.3948e-01,\n",
              "          -3.6519e-01, -2.3308e-01, -4.6742e-01, -1.8450e-01,  1.3686e-01,\n",
              "          -1.2459e-01,  3.7643e-01,  3.3985e-01, -2.6219e-01, -4.7536e-01,\n",
              "           4.5477e-02, -2.3503e-01, -3.2162e-01, -6.2444e-01, -2.9624e-01,\n",
              "           6.9338e-01,  2.4212e-01,  1.2978e-01, -5.9325e-01,  4.9271e-02,\n",
              "          -1.5443e-01, -2.4732e-01, -5.3472e-01,  1.5140e-01,  5.9070e-01,\n",
              "           5.8769e-01,  3.9753e-01,  2.0649e-03,  1.6436e-01,  2.0451e-01,\n",
              "           1.2084e-01,  4.5668e-01, -1.9398e-01,  3.1834e-01, -8.8806e-01,\n",
              "          -7.1856e-01, -2.4682e-01,  3.0049e-01, -1.2930e-02, -1.9802e-01,\n",
              "          -1.5705e-01,  4.6521e-01, -4.0677e-01,  1.6344e-02, -1.8624e-01],\n",
              "         [ 4.2307e-01,  2.3819e-01, -5.2396e-01,  4.6711e-02,  9.5073e-01,\n",
              "           4.4821e-01,  2.9549e-01,  5.3919e-01, -1.3683e-01,  9.0701e-01,\n",
              "          -6.5604e-01, -1.5392e+00, -1.8832e-01,  6.6300e-01,  1.4841e-01,\n",
              "           1.7222e-01,  5.2239e-01,  6.2621e-01,  9.5401e-01,  2.4627e-01,\n",
              "           8.5022e-01, -1.3679e-01, -2.9495e-01,  7.2677e-01,  1.5640e-02,\n",
              "           7.6933e-01, -1.4485e-02,  3.1307e-01,  5.7923e-01, -4.9135e-01,\n",
              "          -1.0915e-01,  2.2979e-01, -7.0290e-01,  3.8559e-01,  3.5688e-01,\n",
              "          -4.1708e-01,  1.1941e+00, -8.5005e-02,  5.0674e-02,  7.9647e-02,\n",
              "          -7.8523e-01, -3.3590e-01, -6.9760e-01, -7.8015e-01, -6.9744e-01,\n",
              "          -4.8456e-01, -6.8678e-01,  8.7580e-01, -2.3028e-01, -4.8658e-01,\n",
              "          -7.7876e-01,  6.9831e-01,  2.4636e-01, -4.7828e-01, -2.9167e-01,\n",
              "          -7.4171e-01, -5.0329e-01,  4.3984e-01, -5.8701e-01, -4.5605e-01,\n",
              "          -2.3391e-01,  2.8026e-01,  1.0350e+00, -8.8176e-01, -3.7451e-01,\n",
              "           5.5922e-02,  9.7116e-02, -4.9994e-01, -8.2135e-01, -6.5057e-01,\n",
              "           5.6132e-01,  3.4459e-01,  1.1408e-01,  3.4396e-01,  1.1444e-02,\n",
              "           2.9153e-02, -5.4901e-01, -7.4826e-01, -2.6096e-01,  4.3694e-01,\n",
              "           2.9568e-01, -4.1959e-02, -2.3133e-01,  3.5010e-01,  3.9852e-01,\n",
              "          -6.1858e-01, -8.4953e-02, -6.5678e-01, -5.0235e-01, -3.1125e-01,\n",
              "          -4.6960e-01,  3.6881e-02, -1.7081e-01,  9.0939e-02,  2.2091e-01,\n",
              "          -3.6451e-01,  6.9959e-01, -9.1766e-02,  2.1846e-01, -5.2288e-01]],\n",
              "\n",
              "        [[-1.6404e-01,  7.0954e-01,  2.1541e-02,  2.7053e-01,  2.9408e-01,\n",
              "           2.4921e-01,  1.1573e-02,  1.2933e-01,  4.3355e-02,  3.5334e-01,\n",
              "          -3.5890e-01, -7.1859e-01, -7.1433e-02,  1.5280e-01, -8.5407e-02,\n",
              "           7.1803e-02,  1.3776e-01,  4.6301e-02,  2.0783e-01,  2.8359e-01,\n",
              "          -5.9212e-02,  3.1677e-01, -4.2963e-01,  4.9672e-01, -1.3236e-01,\n",
              "           3.7397e-01, -1.4169e-01,  8.8839e-02,  4.3661e-01, -3.9288e-02,\n",
              "          -2.6196e-01, -2.4779e-01, -6.9587e-01,  1.3048e-01,  2.6926e-01,\n",
              "           1.5573e-01,  4.5669e-01,  2.5606e-02, -3.2681e-01,  4.8710e-02,\n",
              "          -6.0414e-01, -4.1246e-01, -6.3206e-03, -6.3603e-02, -1.7120e-01,\n",
              "           7.3074e-03, -2.4063e-01,  2.1689e-01,  5.0348e-03, -1.7128e-01,\n",
              "          -3.0293e-01, -4.4003e-02, -6.4347e-02, -3.8343e-02, -7.4598e-02,\n",
              "          -2.3135e-01, -1.6949e-01, -2.8611e-01, -1.5389e-01,  6.8103e-02,\n",
              "          -7.5396e-02,  2.3066e-01,  2.4035e-01, -2.4366e-01, -3.0112e-01,\n",
              "           5.2937e-02, -5.4197e-02, -1.6717e-01, -3.7259e-01, -2.0377e-01,\n",
              "           4.0604e-01,  1.3351e-01,  8.2828e-02, -3.5252e-01,  3.4603e-02,\n",
              "          -1.0728e-01, -1.5938e-01, -3.3816e-01,  1.0415e-01,  3.3774e-01,\n",
              "           3.8178e-01,  1.8914e-01, -4.8434e-02,  1.5861e-01,  1.2345e-01,\n",
              "           1.1437e-01,  3.0677e-01, -7.6114e-02,  2.1198e-01, -4.7521e-01,\n",
              "          -4.5533e-01, -2.1096e-01,  1.5986e-01, -4.9692e-02, -1.1373e-01,\n",
              "          -1.0929e-01,  2.6750e-01, -1.9727e-01,  1.9336e-03, -1.6606e-01],\n",
              "         [ 5.9575e-01, -1.6302e-01, -6.7070e-01, -2.0393e-01,  7.1790e-01,\n",
              "           6.4825e-02,  3.2536e-01,  4.1671e-01, -1.8995e-01,  6.6465e-01,\n",
              "          -3.3956e-01, -8.5830e-01,  9.4471e-02,  6.1266e-01,  2.1479e-01,\n",
              "           4.2451e-02,  4.5078e-01,  3.4822e-01,  8.4274e-01,  1.3885e-01,\n",
              "           7.9149e-01, -4.7563e-01,  2.3354e-01,  4.1026e-01,  1.3429e-01,\n",
              "           5.1261e-01,  1.4384e-01,  9.6497e-02,  2.8635e-01, -3.6157e-01,\n",
              "           1.2198e-01,  2.8429e-01,  1.2262e-01,  2.9267e-01,  1.6609e-01,\n",
              "          -6.4132e-01,  6.5182e-01, -5.8978e-02,  1.2744e-01,  2.7119e-02,\n",
              "          -5.9807e-02,  2.7291e-02, -7.7623e-01, -6.5876e-01, -6.4820e-01,\n",
              "          -6.3271e-01, -3.5293e-01,  6.4534e-01, -3.5027e-01, -2.7682e-01,\n",
              "          -4.3266e-01,  5.5664e-01,  3.4182e-01, -5.0662e-01, -4.4203e-01,\n",
              "          -3.2904e-01, -3.3050e-01,  7.5221e-01, -5.0701e-01, -5.0212e-01,\n",
              "          -2.7339e-01, -1.1475e-01,  5.7546e-01, -6.2991e-01,  3.5019e-02,\n",
              "           1.3452e-01,  2.1416e-01, -2.8880e-01, -4.8464e-01, -5.6623e-01,\n",
              "           2.2558e-01,  1.7597e-01,  1.8237e-01,  5.1192e-01, -2.3023e-01,\n",
              "           1.4018e-01, -2.6060e-01, -6.2761e-01, -1.7819e-01,  1.5701e-01,\n",
              "           2.5866e-01, -3.7892e-01, -2.8582e-01,  2.1752e-01,  2.2006e-01,\n",
              "          -7.4086e-01, -2.3659e-01, -5.9518e-01, -7.0444e-01,  2.3986e-01,\n",
              "          -4.3655e-02,  2.0131e-01, -2.6611e-01,  2.0841e-01,  3.0063e-01,\n",
              "          -1.9750e-01,  3.2114e-01, -8.1715e-02,  9.6460e-02, -4.9960e-01],\n",
              "         [ 3.7777e-01,  6.9407e-01, -2.4687e-01,  1.0859e-01,  6.8243e-01,\n",
              "           4.6393e-01,  1.1882e-01,  5.5039e-01,  1.5699e-02,  9.0260e-01,\n",
              "          -5.4213e-01, -1.4275e+00, -1.6109e-02,  4.6383e-01, -2.8405e-02,\n",
              "           9.1998e-02,  6.1058e-01,  4.3678e-01,  9.7347e-01,  4.0722e-01,\n",
              "           4.5647e-01,  2.9544e-01, -5.8264e-01,  7.9821e-01, -6.5590e-02,\n",
              "           8.3915e-01, -2.5109e-01,  1.1201e-01,  6.6096e-01, -1.2577e-01,\n",
              "          -1.5208e-01, -1.0086e-01, -1.0527e+00,  1.5113e-01,  4.7886e-01,\n",
              "          -1.1806e-01,  1.0007e+00, -1.6991e-01, -3.4712e-01, -2.9169e-02,\n",
              "          -8.9304e-01, -4.0979e-01, -4.1882e-01, -4.9025e-01, -6.6774e-01,\n",
              "          -2.3852e-01, -8.3026e-01,  6.2978e-01, -2.6585e-01, -2.5911e-01,\n",
              "          -6.0471e-01,  5.5095e-01,  7.3109e-02, -5.5936e-01, -4.9449e-01,\n",
              "          -6.4129e-01, -4.2114e-01, -1.3093e-02, -4.0938e-01, -1.9553e-01,\n",
              "          -2.6436e-01,  3.1157e-01,  6.4899e-01, -5.8614e-01, -3.8250e-01,\n",
              "           6.4300e-02, -1.8121e-01, -5.6157e-01, -6.8517e-01, -6.6043e-01,\n",
              "           9.1641e-01,  2.5712e-01,  4.0668e-01, -1.5216e-01, -4.4476e-02,\n",
              "           8.4678e-02, -3.9865e-01, -8.5819e-01,  1.7825e-01,  6.6525e-01,\n",
              "           8.3054e-01,  9.1487e-02, -1.3727e-01,  1.6505e-01,  3.4314e-01,\n",
              "          -4.2988e-01,  2.5515e-01, -6.6385e-01, -1.4607e-01, -8.0660e-01,\n",
              "          -6.6836e-01, -7.2174e-02,  9.6012e-02, -2.2072e-02,  1.2984e-02,\n",
              "          -2.4265e-01,  5.8883e-01, -4.8945e-01,  1.0261e-01, -4.9509e-01],\n",
              "         [ 2.5056e-02,  4.1992e-01, -1.0718e-01,  4.3307e-02,  6.4835e-02,\n",
              "          -8.1009e-03,  5.3486e-02,  7.6390e-02, -5.6298e-02,  2.4414e-01,\n",
              "          -1.0596e-01, -2.8527e-01,  1.9886e-01,  1.1039e-01, -7.9464e-02,\n",
              "          -3.5239e-02,  1.2238e-01, -1.3437e-01,  2.4986e-01,  2.4446e-01,\n",
              "          -2.2443e-02,  8.3547e-02, -9.3583e-02,  3.3206e-01,  5.3137e-03,\n",
              "           2.6708e-01, -7.3000e-02, -8.1832e-02,  3.1171e-01,  1.3469e-01,\n",
              "          -7.9882e-02, -2.4571e-01, -1.5521e-01,  1.2929e-01,  1.9369e-01,\n",
              "          -6.1633e-02,  1.6006e-01,  1.3838e-02, -3.0681e-01, -1.4347e-02,\n",
              "          -8.2950e-02, -1.8706e-01, -1.0157e-01, -2.5498e-03, -2.5939e-01,\n",
              "          -1.4564e-01, -5.7583e-02,  1.6607e-01, -1.5385e-01,  4.5963e-02,\n",
              "          -5.6709e-02, -3.4035e-02,  8.2523e-02, -1.6354e-01, -2.5679e-01,\n",
              "          -2.0028e-03, -7.7782e-02, -3.8925e-02, -1.1041e-01, -7.2776e-03,\n",
              "          -1.1310e-01, -4.8125e-02, -5.8390e-02, -1.1555e-01,  1.1987e-02,\n",
              "           1.0277e-01,  3.8612e-02, -5.9762e-02, -5.9446e-02, -2.0888e-01,\n",
              "           2.3903e-01, -2.3219e-02,  2.3587e-01, -1.6194e-01, -1.3098e-01,\n",
              "           4.5470e-02,  2.8559e-02, -2.8791e-01,  2.3843e-01,  1.4702e-01,\n",
              "           4.5474e-01, -1.2669e-01, -1.1225e-01,  4.3347e-02,  1.4502e-02,\n",
              "          -4.0505e-02,  1.9063e-01, -1.0562e-01,  3.0717e-02, -9.6700e-02,\n",
              "          -1.4788e-01, -9.4425e-02,  5.7476e-02, -3.9878e-02, -3.0958e-02,\n",
              "           7.1648e-03, -1.7370e-02, -2.0758e-01, -7.3668e-02, -2.1460e-01]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "num_hiddens, num_heads = 100, 5\n",
        "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
        "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
        "valid_lens = torch.tensor([3, 2])\n",
        "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
        "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
        "check_shape(attention(X, Y, Y, valid_lens),\n",
        "                (batch_size, num_queries, num_hiddens))"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "rise": {
      "autolaunch": true,
      "enable_chalkboard": true,
      "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
      "scroll": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}