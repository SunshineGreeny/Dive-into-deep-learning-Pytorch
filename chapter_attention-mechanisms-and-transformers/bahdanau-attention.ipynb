{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshineGreeny/Dive-into-deep-learning-Pytorch/blob/main/chapter_attention-mechanisms-and-transformers/bahdanau-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d37bdd",
      "metadata": {
        "id": "e3d37bdd"
      },
      "source": [
        "# The Bahdanau Attention Mechanism\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools"
      ],
      "metadata": {
        "id": "YW9_twDe7Y9t"
      },
      "id": "YW9_twDe7Y9t"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"返回可用的GPU，否则返回CPU\"\"\"\n",
        "    if torch.cuda.device_count() >= i+1:\n",
        "        return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "def bleu(preq_seq,label_seq,k=2):\n",
        "  return sentence_bleu([label_seq.split()],\n",
        "             preq_seq.split(),\n",
        "             smoothing_function=SmoothingFunction().method1,\n",
        "             weights=(1. / k,) * k)\n",
        "\n",
        "\n",
        "def check_shape(tensor, expected_shape):\n",
        "    assert tensor.shape == expected_shape, \\\n",
        "        f\"Expected shape: {expected_shape}, but got: {tensor.shape}\"\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(3.5, 2.5),\n",
        "                  sharex=True,sharey=True,squeeze=False):\n",
        "    num_rows,num_cols=matrices.shape[0],matrices.shape[1]\n",
        "\n",
        "    # 创建子图网络\n",
        "    fig,axes=plt.subplots(num_rows,num_cols,figsize=figsize,\n",
        "                          sharex=True,sharey=True,squeeze=False)\n",
        "    # 遍历所有矩阵并绘制热力图\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            ax=axes[i,j]\n",
        "            # 将张量转换为numpy数组并分离计算图\n",
        "            matrix=matrices[i,j].detach().numpy()\n",
        "\n",
        "            # 使用热力图显示矩阵\n",
        "            pcm=ax.imshow(matrix,cmap='Reds')\n",
        "\n",
        "            # 设置坐标轴标签\n",
        "            if i==num_rows-1:#最后一行显示x轴标签\n",
        "                ax.set_xlabel(xlabel)\n",
        "            if j==0:#第一列显示y轴标签\n",
        "                ax.set_ylabel(ylabel)\n",
        "            if titles:\n",
        "                ax.set_title(titles[j])\n",
        "\n",
        "            # 隐藏刻度线\n",
        "            ax.xaxis.set_ticks_position('none')\n",
        "            ax.yaxis.set_ticks_position('none')\n",
        "\n",
        "    # Add a colorbar\n",
        "    fig.colorbar(pcm, ax=axes.ravel().tolist())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def masked_softmax(X, valid_lens):\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "\n",
        "        # 创建掩码：对于每个位置，如果索引小于有效长度则为True，否则为False\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "\n",
        "        # 将掩码为False的位置设置为指定的值\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    # 没有提供有效长度，直接返回标准sofemax\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "\n",
        "        # 处理不同维度的有效长度\n",
        "        if valid_lens.dim() == 1:\n",
        "            # 如果是一维,复制到每个位置\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "\n",
        "        # 应用序列掩码,将无效位置设置为很小的值\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "\n",
        "        # 应用softmax并恢复原始形状\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
        "\n",
        "\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        # 获取特征维度（用于缩放）\n",
        "        d = queries.shape[-1]\n",
        "\n",
        "        # 计算注意力分数：Q * K^T / √d\n",
        "        # bmm: 批量矩阵乘法\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "\n",
        "        # 应用带掩码的softmax得到注意力权重\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "\n",
        "        # 对注意力权重应用dropout，然后与值相乘得到最终输出\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = DotProductAttention(dropout)\n",
        "        self.W_q = nn.Linear(num_hiddens, num_hiddens, bias=bias) # Changed LazyLinear to Linear\n",
        "        self.W_k = nn.Linear(num_hiddens, num_hiddens, bias=bias) # Changed LazyLinear to Linear\n",
        "        self.W_v = nn.Linear(num_hiddens, num_hiddens, bias=bias) # Changed LazyLinear to Linear - Note: W_v should map to num_hiddens for consistency before final W_o\n",
        "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias) # Changed LazyLinear to Linear\n",
        "\n",
        "    def transpose_qkv(self, X):\n",
        "        \"\"\"Transposes the last two dimensions of a tensor.\"\"\"\n",
        "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "        X = X.permute(0, 2, 1, 3)\n",
        "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "    def transpose_output(self, X):\n",
        "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
        "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "        X = X.permute(0, 2, 1, 3)\n",
        "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
        "\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries = self.transpose_qkv(self.W_q(queries))\n",
        "        keys = self.transpose_qkv(self.W_k(keys))\n",
        "        values = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "        if valid_lens is not None:\n",
        "            valid_lens = torch.repeat_interleave(\n",
        "                valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        output_concat = self.transpose_output(output)\n",
        "        return self.W_o(output_concat)\n",
        "\n",
        "\n",
        "class MTFraEng(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sentence = self.src_sentences[idx]\n",
        "        tgt_sentence = self.tgt_sentences[idx]\n",
        "\n",
        "        src_tokens = [''] + src_sentence.split() + ['']\n",
        "        tgt_tokens = [''] + tgt_sentence.split() + ['']\n",
        "\n",
        "        src_indices = [self.src_vocab[token] for token in src_tokens if token in self.src_vocab]\n",
        "        tgt_indices = [self.tgt_vocab[token] for token in tgt_tokens if token in self.tgt_vocab]\n",
        "\n",
        "        # 填充或截断到固定长度\n",
        "        src_indices = self.pad_or_truncate(src_indices, self.max_len, self.src_vocab[''])\n",
        "        tgt_indices = self.pad_or_truncate(tgt_indices, self.max_len, self.tgt_vocab[''])\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "    def pad_or_truncate(self, sequence, max_len, pad_token):\n",
        "        if len(sequence) < max_len:\n",
        "            return sequence + [pad_token] * (max_len - len(sequence))\n",
        "        else:\n",
        "            return sequence[:max_len]\n",
        "\n",
        "    def build_vocab(sentences, min_freq=2):\n",
        "        tokens = []\n",
        "        for sentence in sentences:\n",
        "            tokens.extend(sentence.split())\n",
        "\n",
        "        token_freq = {}\n",
        "        for token in tokens:\n",
        "            token_freq[token] = token_freq.get(token, 0) + 1\n",
        "\n",
        "        vocab = {'': 0, '': 1, '': 2}\n",
        "        for token, freq in token_freq.items():\n",
        "            if freq >= min_freq:\n",
        "                vocab[token] = len(vocab)\n",
        "\n",
        "        # Add get_itos method for demonstration\n",
        "        vocab['get_itos'] = lambda: {v: k for k, v in vocab.items()}\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def build(self, src_sentences, tgt_sentences):\n",
        "        \"\"\"Builds the dataset for prediction.\"\"\"\n",
        "        src_indices = []\n",
        "        tgt_indices = []\n",
        "        for src_sentence, tgt_sentence in zip(src_sentences, tgt_sentences):\n",
        "            src_tokens = [''] + src_sentence.split() + ['']\n",
        "            tgt_tokens = [''] + tgt_sentence.split() + ['']\n",
        "            src_indices.append([self.src_vocab[token] for token in src_tokens if token in self.src_vocab])\n",
        "            tgt_indices.append([self.tgt_vocab[token] for token in tgt_tokens if token in self.tgt_vocab])\n",
        "\n",
        "        # Pad or truncate to max_len\n",
        "        src_indices = [self.pad_or_truncate(indices, self.max_len, self.src_vocab['']) for indices in src_indices]\n",
        "        tgt_indices = [self.pad_or_truncate(indices, self.max_len, self.tgt_vocab['']) for indices in tgt_indices]\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding.\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
        "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
        "            -1, 1) / torch.pow(10000, torch.arange(\n",
        "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
        "        self.P[:, :, 0::2] = torch.sin(X)\n",
        "        self.P[:, :, 1::2] = torch.cos(X)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
        "        return self.dropout(X)\n",
        "\n",
        "\n",
        "def Trainer(model, train_loader, val_loader, num_epochs, lr, device):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略填充token\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for src, tgt in train_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            enc_outputs, _ = model.encoder(src)\n",
        "            state = model.decoder.init_state(enc_outputs, None)\n",
        "\n",
        "            # 解码器输入是目标序列去掉最后一个token\n",
        "            dec_input = tgt[:, :-1]\n",
        "            # 解码器输出应该预测目标序列去掉第一个token\n",
        "            dec_output, _ = model.decoder(dec_input, state)\n",
        "\n",
        "            loss = criterion(dec_output.reshape(-1, dec_output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # 验证阶段\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in val_loader:\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "                enc_outputs, _ = model.encoder(src)\n",
        "                state = model.decoder.init_state(enc_outputs, None)\n",
        "                dec_input = tgt[:, :-1]\n",
        "                dec_output, _ = model.decoder(dec_input, state)\n",
        "\n",
        "                loss = criterion(dec_output.reshape(-1, dec_output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # 绘制损失曲线\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, num_epochs+1), val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, tgt_vocab):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        enc_outputs = self.encoder(src)\n",
        "        dec_state = self.decoder.init_state(enc_outputs, None)\n",
        "        dec_input = tgt[:, :-1]\n",
        "        logits, _ = self.decoder(dec_input, dec_state)\n",
        "        return logits\n",
        "\n",
        "    def predict_step(self, src_seqs, device, max_len, save_attention_weights=False):\n",
        "        \"\"\"Predict the output sequence.\"\"\"\n",
        "        self.eval()\n",
        "        batch_size = len(src_seqs)\n",
        "        enc_outputs, _ = self.encoder(src_seqs.to(device))\n",
        "        dec_state = self.decoder.init_state(enc_outputs, None)\n",
        "        # Start with the  token\n",
        "        dec_X = torch.tensor([self.tgt_vocab['']] * batch_size,\n",
        "                             device=device).unsqueeze(1)\n",
        "        preds = []\n",
        "        attention_weights = []\n",
        "        for _ in range(max_len):\n",
        "            Y, dec_state = self.decoder(dec_X, dec_state)\n",
        "            # Use the token with the highest predicted probability as the next\n",
        "            # input to the decoder\n",
        "            dec_X = Y.argmax(dim=2)\n",
        "            preds.extend(dec_X.squeeze(1).tolist())\n",
        "            # Save attention weights (optional)\n",
        "            if save_attention_weights:\n",
        "                attention_weights.append(self.decoder.attention_weights)\n",
        "        return torch.tensor(preds).reshape(batch_size, -1), attention_weights\n",
        "\n",
        "\n",
        "class AddictiveAttention(nn.Module):\n",
        "    def __init__(self, num_hiddens, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(num_hiddens, num_hiddens, bias=False)\n",
        "        self.W_k = nn.Linear(num_hiddens, num_hiddens, bias=False)\n",
        "        self.W_v = nn.Linear(num_hiddens, 1, bias=False) # Changed output features to 1 for the final scoring\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        # queries shape: (batch_size, num_hiddens) - assuming query_len = 1\n",
        "        # keys shape: (batch_size, key_len, num_hiddens)\n",
        "        # values shape: (batch_size, key_len, num_hiddens)\n",
        "\n",
        "        # Apply linear transformations\n",
        "        # queries_transformed shape: (batch_size, num_hiddens)\n",
        "        queries_transformed = self.W_q(queries)\n",
        "        # keys_transformed shape: (batch_size, key_len, num_hiddens)\n",
        "        keys_transformed = self.W_k(keys)\n",
        "\n",
        "        # Expand dimensions for broadcasting and addition\n",
        "        # queries_transformed_expanded: (batch_size, 1, num_hiddens)\n",
        "        # keys_transformed_expanded: (batch_size, key_len, num_hiddens) -> (batch_size, 1, key_len, num_hiddens)\n",
        "        # We need to expand queries to match key_len dimension and keys to match query_len dimension (which is 1)\n",
        "        # The sum should be (batch_size, 1, key_len, num_hiddens)\n",
        "        features = queries_transformed.unsqueeze(1).unsqueeze(2) + keys_transformed.unsqueeze(1)\n",
        "\n",
        "\n",
        "        # Apply the tanh and W_v layer, then squeeze the last dimension\n",
        "        # features: (batch_size, 1, key_len, num_hiddens) -> (batch_size, 1, key_len, 1)\n",
        "        # scores: (batch_size, 1, key_len)\n",
        "        scores = self.W_v(torch.tanh(features)).squeeze(-1)\n",
        "\n",
        "        if valid_lens is not None:\n",
        "             # Need to handle valid_lens masking for additive attention scores (batch_size, 1, key_len)\n",
        "            # The mask should be applied to the last dimension (key_len)\n",
        "            mask = torch.arange(keys.shape[1], device=keys.device).unsqueeze(0).unsqueeze(0) >= valid_lens.unsqueeze(-1)\n",
        "            scores = scores.masked_fill(mask, -1e6)\n",
        "\n",
        "\n",
        "        self.attention_weights = F.softmax(scores, dim=-1) # Shape: (batch_size, 1, key_len)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values) # Result shape: (batch_size, 1, num_hiddens)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def init_state(self,enc_outputs,*args):\n",
        "    raise NotImplementedError\n",
        "  def forward(self,X,state):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "class Seq2SeqEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers,\n",
        "                           dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.embedding(X)\n",
        "        output, state = self.rnn(X)\n",
        "        return output, state  # output: (B, T, H), state: (h_n, c_n)\n",
        "\n",
        "\n",
        "def init_seq2seq(src_vocab_size, tgt_vocab_size, embed_size=256, num_hiddens=256, num_layers=2, dropout=0.1):\n",
        "    encoder = Seq2SeqEncoder(src_vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
        "    decoder = Seq2SeqAttentionDecoder(tgt_vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
        "    return Seq2Seq(encoder, decoder)"
      ],
      "metadata": {
        "id": "GBEdEM_K7aRG"
      },
      "id": "GBEdEM_K7aRG",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7a2c46a",
      "metadata": {
        "id": "b7a2c46a"
      },
      "source": [
        "The base interface for decoders with attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7392fc80",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:46:16.115524Z",
          "iopub.status.busy": "2023-08-18T19:46:16.114277Z",
          "iopub.status.idle": "2023-08-18T19:46:16.121848Z",
          "shell.execute_reply": "2023-08-18T19:46:16.120397Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "7392fc80"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(Decoder):\n",
        "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def attention_weights(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a94603f",
      "metadata": {
        "id": "7a94603f"
      },
      "source": [
        "Implement the RNN decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f0a3f536",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:46:16.128312Z",
          "iopub.status.busy": "2023-08-18T19:46:16.125898Z",
          "iopub.status.idle": "2023-08-18T19:46:16.142962Z",
          "shell.execute_reply": "2023-08-18T19:46:16.141775Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "f0a3f536"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.attention = AddictiveAttention(num_hiddens, dropout)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(\n",
        "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
        "            dropout=dropout, batch_first=False) # Ensure batch_first is False\n",
        "\n",
        "        self.dense = nn.LazyLinear(vocab_size)\n",
        "        # Removed: self.apply(init_seq2seq)\n",
        "\n",
        "    def init_state(self, enc_outputs, enc_valid_lens):\n",
        "        outputs, hidden_state = enc_outputs\n",
        "        # Corrected: Do not permute outputs\n",
        "        return (outputs, hidden_state, enc_valid_lens)\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        enc_outputs, hidden_state, enc_valid_lens = state\n",
        "        X = self.embedding(X).permute(1, 0, 2) # Permute to (seq_len, batch_size, embed_size) for GRU\n",
        "        outputs, self._attention_weights = [], []\n",
        "        for x in X:\n",
        "            # Pass the hidden state directly as the query\n",
        "            query = hidden_state[-1] # Shape: (batch_size, num_hiddens)\n",
        "            context = self.attention(\n",
        "                query, enc_outputs, enc_outputs, enc_valid_lens) # enc_outputs shape: (batch_size, seq_len, num_hiddens)\n",
        "\n",
        "            # Reshape context to (1, batch_size, num_hiddens) to match GRU input expectation\n",
        "            context = context.unsqueeze(0)\n",
        "\n",
        "            # x shape: (batch_size, embed_size)\n",
        "            # Unsqueeze x to (1, batch_size, embed_size) to match GRU input expectation\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "            # Concatenate context and embedded input\n",
        "            rnn_input = torch.cat((context, x), dim=-1) # Shape: (1, batch_size, num_hiddens + embed_size)\n",
        "\n",
        "            out, hidden_state = self.rnn(rnn_input, hidden_state) # out shape: (1, batch_size, num_hiddens), hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
        "            outputs.append(out)\n",
        "            self._attention_weights.append(self.attention.attention_weights)\n",
        "\n",
        "        # Concatenate outputs from all time steps\n",
        "        outputs = torch.cat(outputs, dim=0) # Shape: (seq_len, batch_size, num_hiddens)\n",
        "\n",
        "        # Apply dense layer (LazyLinear expects last dim as input)\n",
        "        outputs = self.dense(outputs) # Shape: (seq_len, batch_size, vocab_size)\n",
        "\n",
        "        # Permute back to (batch_size, seq_len, vocab_size)\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "        return outputs, [enc_outputs, hidden_state,\n",
        "                                          enc_valid_lens]\n",
        "\n",
        "    @property\n",
        "    def attention_weights(self):\n",
        "        # Assuming attention_weights are collected per step and have shape (batch_size, 1, key_len)\n",
        "        # Concatenate along the query_len dimension (dim 1 after unsqueezing in attention)\n",
        "        # The stored weights are already (batch_size, 1, key_len), concatenating a list of these\n",
        "        # will result in (seq_len, batch_size, 1, key_len).\n",
        "        # We need to reshape or permute to get (batch_size, seq_len, key_len)\n",
        "        if self._attention_weights:\n",
        "            # Stack the list of tensors\n",
        "            stacked_weights = torch.stack(self._attention_weights, dim=1) # Shape: (batch_size, seq_len, 1, key_len)\n",
        "            # Squeeze the dimension of size 1\n",
        "            return stacked_weights.squeeze(2) # Shape: (batch_size, seq_len, key_len)\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4db11b4",
      "metadata": {
        "id": "b4db11b4"
      },
      "source": [
        "Test the implemented\n",
        "decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e7d6e370",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:46:16.149119Z",
          "iopub.status.busy": "2023-08-18T19:46:16.148126Z",
          "iopub.status.idle": "2023-08-18T19:46:16.201990Z",
          "shell.execute_reply": "2023-08-18T19:46:16.200456Z"
        },
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "e7d6e370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "c7bccb6b-176a-4aaa-c486-0ce0555977d0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3552026398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-486550714.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Pass the hidden state directly as the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Shape: (batch_size, num_hiddens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             context = self.attention(\n\u001b[0m\u001b[1;32m     27\u001b[0m                 query, enc_outputs, enc_outputs, enc_valid_lens) # enc_outputs shape: (batch_size, seq_len, num_hiddens)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1426785027.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, queries, keys, values, valid_lens)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# We need to expand queries to match key_len dimension and keys to match query_len dimension (which is 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# The sum should be (batch_size, 1, key_len, num_hiddens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeys_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 3"
          ]
        }
      ],
      "source": [
        "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
        "batch_size, num_steps = 4, 7\n",
        "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
        "decoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\n",
        "                                  num_layers)\n",
        "X = torch.zeros((batch_size, num_steps), dtype=torch.long)\n",
        "state = decoder.init_state(encoder(X), None)\n",
        "output, state = decoder(X, state)\n",
        "check_shape(output, (batch_size, num_steps, vocab_size))\n",
        "check_shape(state[0], (batch_size, num_steps, num_hiddens))\n",
        "check_shape(state[1][0], (batch_size, num_hiddens))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b91a2a1f",
        "outputId": "4eba500d-dc61-407f-fd15-e18e69a7a4cf"
      },
      "source": [
        "# Placeholder code to load and preprocess data\n",
        "\n",
        "# In a real scenario, you would load your French and English sentences\n",
        "# from a file or another source.\n",
        "src_sentences = [\"This is a test sentence .\", \"Another example .\"]\n",
        "tgt_sentences = [\"Ceci est une phrase de test .\", \"Un autre exemple .\"]\n",
        "\n",
        "# Build vocabularies\n",
        "# Assuming build_vocab method is available in the MTFraEng class or elsewhere\n",
        "# If not, you would need to implement vocabulary building.\n",
        "src_vocab = MTFraEng.build_vocab(src_sentences)\n",
        "tgt_vocab = MTFraEng.build_vocab(tgt_sentences)\n",
        "\n",
        "# Determine max_len based on your dataset or a predefined value\n",
        "max_len = 10  # Example value\n",
        "\n",
        "# Now initialize the dataset correctly\n",
        "data = MTFraEng(src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len)\n",
        "\n",
        "# The rest of the code from cell a73f9cc6 can follow here\n",
        "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
        "encoder = Seq2SeqEncoder(\n",
        "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqAttentionDecoder(\n",
        "    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "model = Seq2Seq(encoder, decoder, tgt_vocab=data.tgt_vocab) # Pass tgt_vocab\n",
        "# The Trainer class is not defined in the provided code. Assuming a Trainer class exists and is imported or defined elsewhere.\n",
        "# Placeholder for Trainer initialization and fitting.\n",
        "# trainer = Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
        "# trainer.fit(model, data)\n",
        "\n",
        "print(\"Placeholder data loaded and model initialized.\")"
      ],
      "id": "b91a2a1f",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Placeholder data loaded and model initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6016043c",
      "metadata": {
        "id": "6016043c"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a73f9cc6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:46:16.207768Z",
          "iopub.status.busy": "2023-08-18T19:46:16.207253Z",
          "iopub.status.idle": "2023-08-18T19:46:52.077164Z",
          "shell.execute_reply": "2023-08-18T19:46:52.076268Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "a73f9cc6",
        "outputId": "a8cc8f4d-c83d-47a6-e015-f3cf73a39cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (10) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2473965014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1426785027.py\u001b[0m in \u001b[0;36mTrainer\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;31m# 解码器输出应该预测目标序列去掉第一个token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-486550714.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Pass the hidden state directly as the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Shape: (batch_size, num_hiddens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             context = self.attention(\n\u001b[0m\u001b[1;32m     27\u001b[0m                 query, enc_outputs, enc_outputs, enc_valid_lens) # enc_outputs shape: (batch_size, seq_len, num_hiddens)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1426785027.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, queries, keys, values, valid_lens)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# We need to expand queries to match key_len dimension and keys to match query_len dimension (which is 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# The sum should be (batch_size, 1, key_len, num_hiddens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeys_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (10) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "# data = MTFraEng(batch_size=128) # Removed incorrect initialization\n",
        "\n",
        "# Assume src_sentences, tgt_sentences, src_vocab, tgt_vocab, and max_len are defined from previous steps\n",
        "# Re-initialize data, encoder, decoder, and model using the preprocessed data\n",
        "data = MTFraEng(src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len)\n",
        "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
        "encoder = Seq2SeqEncoder(\n",
        "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqAttentionDecoder(\n",
        "    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "model = Seq2Seq(encoder, decoder, tgt_vocab=data.tgt_vocab)\n",
        "\n",
        "# Assuming a Trainer class is defined elsewhere or you have implemented a training loop\n",
        "# Placeholder for Trainer initialization and fitting.\n",
        "# You would need to replace this with your actual training code.\n",
        "# For now, I'll use the Trainer function defined in GBEdEM_K7aRG\n",
        "device = try_gpu()\n",
        "lr = 0.005 # Assuming a learning rate\n",
        "num_epochs = 30 # Assuming number of epochs\n",
        "train_loader = DataLoader(data, batch_size=128, shuffle=True) # Assuming batch size\n",
        "# Create a dummy validation loader for now\n",
        "val_loader = DataLoader(data, batch_size=128) # Replace with actual validation data\n",
        "\n",
        "\n",
        "Trainer(model, train_loader, val_loader, num_epochs, lr, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "085c7e96",
      "metadata": {
        "id": "085c7e96"
      },
      "source": [
        "Translate a few English sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "a22c2296",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:46:52.082122Z",
          "iopub.status.busy": "2023-08-18T19:46:52.081526Z",
          "iopub.status.idle": "2023-08-18T19:46:52.108612Z",
          "shell.execute_reply": "2023-08-18T19:46:52.107700Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "a22c2296",
        "outputId": "7eb5feff-8ccf-4ecd-f4c6-86861c432810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MTFraEng' object has no attribute 'num_steps'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3458784906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'va !'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'j\\'ai perdu .'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'il est calme .'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'je suis chez moi .'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m preds, _ = model.predict_step(\n\u001b[0;32m----> 4\u001b[0;31m     data.build(engs, fras), try_gpu(), data.num_steps)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MTFraEng' object has no attribute 'num_steps'"
          ]
        }
      ],
      "source": [
        "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
        "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
        "preds, _ = model.predict_step(\n",
        "    data.build(engs, fras), try_gpu(), data.num_steps)\n",
        "for en, fr, p in zip(engs, fras, preds):\n",
        "    translation = []\n",
        "    for token in data.tgt_vocab.to_tokens(p):\n",
        "        if token == '<eos>':\n",
        "            break\n",
        "        translation.append(token)\n",
        "    print(f'{en} => {translation}, bleu,'\n",
        "          f'{bleu(\" \".join(translation), fr, k=2):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7470cbee",
      "metadata": {
        "id": "7470cbee"
      },
      "source": [
        "Visualize the attention weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "e9c665a8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:46:52.134058Z",
          "iopub.status.busy": "2023-08-18T19:46:52.133495Z",
          "iopub.status.idle": "2023-08-18T19:46:52.369882Z",
          "shell.execute_reply": "2023-08-18T19:46:52.368741Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "e9c665a8",
        "outputId": "38d243f0-c107-4091-caf0-c5075c63801d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MTFraEng' object has no attribute 'num_steps'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1267083018.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m _, dec_attention_weights = model.predict_step(\n\u001b[0;32m----> 2\u001b[0;31m     data.build([engs[-1]], [fras[-1]]), try_gpu(), data.num_steps, True)\n\u001b[0m\u001b[1;32m      3\u001b[0m attention_weights = torch.cat(\n\u001b[1;32m      4\u001b[0m     [step[0][0][0] for step in dec_attention_weights], 0)\n\u001b[1;32m      5\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MTFraEng' object has no attribute 'num_steps'"
          ]
        }
      ],
      "source": [
        "_, dec_attention_weights = model.predict_step(\n",
        "    data.build([engs[-1]], [fras[-1]]), try_gpu(), data.num_steps, True)\n",
        "attention_weights = torch.cat(\n",
        "    [step[0][0][0] for step in dec_attention_weights], 0)\n",
        "attention_weights = attention_weights.reshape((1, 1, -1, data.num_steps))\n",
        "\n",
        "show_heatmaps(\n",
        "    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),\n",
        "    xlabel='Key positions', ylabel='Query positions')"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "rise": {
      "autolaunch": true,
      "enable_chalkboard": true,
      "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
      "scroll": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}